* Converting your Notebook into a training script
Notebooks are perfect for rapid development, but they can be difficult to collaborate on, and hard to automate, making them ill-suited for MLOps. After having become familiar with the Azure ML basic, we are gonna convert our notebook to a simple training script that we will run both locally, and remotely.

** Setting up a non-interactive Run
Create a directory called "src" in your repository. Create a single file called train.py there. Extract the necessary code from your notebook into the train.py file. It should connect to your Azure ML workspace, and download the necessary dataset, as you did before.

Make sure that you can run the script as src_bash{python src/train.py}
and that it runs until completion, just like in your notebook.

Commit this code, because we are now gonna start breaking things. Create a second folder called *ml_pipelines* in the project root. This folder will contain orchestration code for our training scripts. "Orchestration" code in Azure ML is SDK code that manages the execution of our experiments. It defines the environment for the scripts to run in, provisions compute for the experiments, and can also manage how models are deployed.

Create a script in *ml_pipelines* called *run_train_script.py*. For now, this script only needs to define a python run step and the environment it should run in. To make debugging easier we start by running it locally. We can reuse the environment we used for deployment earlier.

#+begin_src python
# ml_pipeline/run_train_script
from azureml.core import Environment, Workspace, Experiment
from ml_pipelines.utils import get_environment
ws = Workspace.from_config()
environment = get_environment()
#+end_src

Then, create a ScriptConfig that references the extracted training script
#+begin_src python
# ml_pipelines/run_train_script.py
from azureml.core import ScriptRunConfig
# ...
src = ScriptRunConfig(source_directory='src', script='train.py', environment=environment)
run = experiment.submit(src)
run.wait_for_completion(show_output=True)
#+end_src

Before we run this experiment, you need to modify *train.py* script with two simple changes.
*Replace* the creation of the workspace, experiment, and the run in your script
#+begin_src python
# src/train.py (old)
ws = Workspace.from_config()
experiment = Experiment(ws, "some-experiment-name")
# ...
run = experiment.start_logging()
#...
run.complete()
#+end_src

with this

#+begin_src python
# src/train.py (new)
from azureml.core import Run
# ...
run = Run.get_context()
experiment = run.experiment
workspace = run.experiment.workspace
#+end_src

This change is required because now the orchestration code is responsible for instantiating the run, and the script gets those objects from the Run context instead.

Execute the new orchestration code to start the run
#+begin_src bash
python -m ml_pipelines.run_train_script.py
#+end_src
this should now start a new run in a Docker environment on your local machine. If you go to the Studio you should still see a new run having started on the experiment.

** Using AML Compute
Now, we should look into how to allocate remote compute in Azure ML. A small CPU cluster is often enough for simple training scripts The following code

#+begin_src python
# ml_pipelines/run_train_script.py
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException
# ...
cpu_cluster_name = "my-cool-cluster"
try:
    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(
        vm_size="STANDARD_D2_V2",
        max_nodes=1,
        idle_seconds_before_scaledown=1200, # Scale down after 20 minutes
    )
    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)
#+end_src

Add it as the compute_target for the ScriptRunConfig
#+begin_src python
src = ScriptRunConfig('src', script='train.py', environment=local_env, compute_target=cpu_cluster)
#+end_src

Run the orchestration code again. If you look at the *compute* section in the Studio, and under compute clusters, you should see that a new cluster is being created. This can take a couple of minutes. You can check the experiment section and see if a run has started. In the logs, you can start to see how the docker is being built, and how they run is started.

* Convert your Notebook to an Azure ML Pipeline
Scripts are nice, but it's a hassle to have to resubmit a new script everytime if we just want to run training again against e.g., some new data.  Pipelines allows us to keep a definition of one or more scripts in the cloud, which we can then invoke using new data.

Azure ML Pipelines are a series of python scripts that runs in a given order. Each step can run on separate compute and in separate environments. For example, you can use a small cluster for data preprocessing, and then a GPU-enabled compute instance to train your large deep learning models, without incurring excessive cost.

However, they can be annoying to debug since they (for some reason) can't run on your local computer, and need to be submitted to Azure ML.

Pipelines also allow us to use PipelineParameters, which can be set when calling a published pipeline. This is useful if you have training code that doesn't change much, but you regularly register a new dataset on which you want to retrain your model.

Pipelines are very similar to ScriptRunConfigs, with some minor changes. First, there is a slight difference in how Runs work in pipelines. The pipeline itself has a run each step is a child run with its own run id. For convenience, it is nice to log metrics both in the child and the parent-run. Similarly, it is often better to register the model to the parent-run, rather than the child run, since it makes it easier to inspect the logs in the Studio. Get the parent run and replace the normal run.log with parent_run.log
#+begin_src python
# src/train.py
parent_run = run.parent
parent_run.log(...)
#+end_src

Then, create a new orchestration script ml_pipelines/build_pipeline.py and copy the contents of the run_train_script. Replace the script config definition in the build_pipeline file.
#+begin_src python
# ml_pipelines/build_pipeline.py (old)
from azureml.core import ScriptRunConfig
# ...
src = ScriptRunConfig('src', script='train.py', environment=environment)
run = experiment.submit(src)
run.wait_for_completion(show_output=True)
#+end_src

to a Python Script Step and a pipeline
#+begin_src python
# ml_pipelines/build_pipeline.py (new)
run_config = RunConfiguration()
# Remember to set our favorite environment
run_config.environment = environment

train_step = PythonScriptStep(
    name="training_step",
    script_name="train.py",
    source_directory="src",
    compute_target=cpu_cluster,
    runconfig=run_config,
    allow_reuse=False
)

pipeline = Pipeline(
    workspace=workspace, steps=[train_step], description="Model Training and Deployment"
)
pipeline.validate()

pipeline_name = <some-good-pipeline-name>
published_pipeline = pipeline.publish(pipeline_name)
#+end_src

#+begin_src python
python -m ml_pipelines.build_pipeline.py
#+end_src

The pipeline object now exists as a callable object your workspace and can easily be executed whenever we see fit, regardless if the code change or not. We can then simply add a new orchestration script like this to invoke the pipeline with the newest version of the datasets we defined earlier.

#+begin_src python
# ml_pipelines/run_pipeline.py
from azureml.core import Experiment
from azureml.pipeline.core import Pipeline

workspace = Workspace.from_config()
pipeline = Pipeline.get(workspace, <pipeline-name>)
experiment = Experiment(workspace, <name-of-your-experiment>)
experiment.submit(pipeline)
#+end_src

* Advanced ML Pipeline - Optional
There are some more advanced options for how to build pipelines

** Configuring Inputs
Pipelines offer a different way of defining input data. You can define

#+begin_src python
# ml_pipelines/build_pipeline.py
train_dataset = Dataset.get_by_name(
    workspace, "train-dataset"
)
test_dataset = Dataset.get_by_name(
    workspace, "test-dataset"
)
train_ds_consumption = DatasetConsumptionConfig("train_ds", train_dataset)
test_ds_consumption = DatasetConsumptionConfig("test_ds", test_dataset)
inputs = [train_ds_consumption, test_ds_consumption]
#+end_src
and add it to the PythonScriptStep.

Then, instead of referencing the dataset by name in the training step, we use the input_datasets dictionary from the run. Other than that, things stay the same.
#+begin_src python
# src/train.py
datasets = run.input_datasets
train_dataset = datasets["train_ds"]
test_dataset = datasets["test_ds"]
#+end_src
