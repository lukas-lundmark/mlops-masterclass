* Converting your Notebook into a training script
Notebooks are perfect for rapid development, but they can be difficult to collaborate on, and hard to automate, and also morechallenging to reproduce, making them ill-suited for MLOps. After having become familiar with Azure ML basic, having both created a small protoype model and having deployed a model, we will now convert our notebook to a training script, such that we can start to automate the retraining process, moving closer to a MLOps Level 1 setup.

** Setting up a non-interactive Run
Create a single file called ~train.py~ in the ~src~ directory. Extract the necessary code from your notebook into the train.py file. It should connect to your Azure ML workspace, download the necessary dataset, and train and register the model, as you did before. You can discard the visualization since it is not necessary.

Make sure that you can run the script as src_bash{python src/train.py}
and that it can run without failing, just like in your notebook.

Now, you might want to commit your code, because we are now gonna start breaking things.

Create a script in ~ml_pipelines~ called ~run_train_script.py~. For now, this script only needs to define a python run step and the environment it should run in. To make debugging easier we start by running it locally. We can reuse the environment we used for deployment earlier.

#+begin_src python
# ml_pipeline/run_train_script
from azureml.core import Environment, Workspace, Experiment
from ml_pipelines.utils import get_environment
ws = Workspace.from_config()
environment = get_environment()
experiment = Experiment(ws, "some-experiment-name")
#+end_src

Then, create a ScriptConfig that references the extracted training script
#+begin_src python
# ml_pipelines/run_train_script.py
from azureml.core import ScriptRunConfig
# ...
src = ScriptRunConfig(source_directory='src', script='train.py', environment=environment)
run = experiment.submit(src)
run.wait_for_completion(show_output=True)
#+end_src
You might notices that this shares many similarities with how we defined deployments - specify the scripts we want to run, and the environment we want to run them in.

Before we run this experiment, you need to modify *train.py* script a little bit compared to before. Replace the creation of the workspace, experiment, and the run in your script. Replace these calls
#+begin_src python
# src/train.py (old)
ws = Workspace.from_config()
experiment = Experiment(ws, "some-experiment-name")
# ...
run = experiment.start_logging()
#...
run.complete()
#+end_src

with this

#+begin_src python
# src/train.py (new)
from azureml.core import Run
# ...
run = Run.get_context()
experiment = run.experiment
workspace = run.experiment.workspace
#+end_src

This change is required because now the orchestration code is responsible for starting the run, and the script receives those objects from the Run context instead. Note that we also removed the src_python{run.complete(), since AML now ends the run for us.

Execute the new orchestration code to start the run.
#+begin_src bash
python -m ml_pipelines.run_train_script.py
#+end_src
This should start a new run in a Docker environment on your local machine. If you go to the Studio you should still see a new run having started on the experiment.

** Using AML Compute
Now, we will again allocate some compute resources in Azure ML to run our code. A small CPU cluster is often enough for our very basic training scripts. Especially if you are using a free Azure subscription, since they put strict limitations on the number of vCores you can use.

#+begin_src python
# ml_pipelines/run_train_script.py
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException
# ...
cpu_cluster_name = "my-cool-cluster"
try:
    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(
        vm_size="STANDARD_D2_V2",
        max_nodes=1,
        idle_seconds_before_scaledown=1200, # Scale down after 20 minutes
    )
    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)
    # This will block the script until the resource is created
    cpu_cluster.wait_for_complete(show_output=True)
#+end_src

Note the usual creation pattern, with a try and except clause. We also block until the resource has been created, which takes up to five minutes.

Set the new cpu_cluster as the compute_target for the ScriptRunConfig
#+begin_src python
# ml_pipelines/run_train_script.py
src = ScriptRunConfig(
    name='src',
    script='train.py',
    environment=local_env,
    compute_target=cpu_cluster
)
#+end_src

Run the orchestration code again. If you look at the *compute* section in the Studio, and under compute clusters, you should see that a new cluster is being created. Then, you will see it starting to scale up from zero cores to one. This process can take a couple of minutes.

After the compute resource has been created, you experiment will run. Before that, it will be queued. You can check the experiment section and see the status of the run. If you are feeling bored, you can inspect the logs of your run. You should start to see how the docker container that will exectute your code is being built.

Note that it will take much quicker to run the experiment a second time. The environment will note have to be created again, and if you run the script before the cluster scales down it should start almost immediately.


** Sending Arguments to the Script
A lot of values in the original notebook, such as dataset and model names, where hard coded in the original notebook. This works okay in notebooks, since they are meant to be interactive and are easy to change. However, in scripts, we generally want such values to be modifiable using arguments. Python's default argparse module will suffice

Write something like this to replace the hard coded values for the train and test dataset names
#+begin_src python
# scr/train.py
import argparse

parser = argparse.ArgumentParser("This")
parser.add_argument('--train-dataset', default='diamond-train')
parser.add_argument('--test-dataset', default='diamond-test')
parser.add_argument('--model-name', default='diamond-regressor')
args = paser.parse_args()

train_ds = args.train_dataset
test_ds = args.test_dataset
model_name = args.model_name
# ...
#+end_src

#+RESULTS:

If you would run it locally, you would then invoke the script like this
#+begin_src bash
python src/train.py --model-name new-diamond-regressor
#+end_src

Similarly, you can update the ScriptRunConfig to take one or more arguments in the orchestration script.

#+begin_src python
# ml_pipelines/run_train_script.py
src = ScriptRunConfig(
    name='src',
    script='train.py',
    environment=local_env,
    compute_target=cpu_cluster
    arguments= ['--model-name', 'new-diamond-regressor']
)
#+end_src
This is a good place to use environment variables to make it so we can dynamically change the name of the model or datasets by changing the values in our ~.env~.

Finish this new the orchestration script with a new value for the model. If you look at the Run in the studio you should be able to see exactly which values where provided to the script when it was invoked, another useful tool when reproducing earlier experiments. This should register a new model

In the next section, we will convert this single script to persistant pipeline that we can invoke without having to resubmit new code each time.

* Convert your Notebook to an Azure ML Pipeline
Scripts are nice, but it's a hassle having to resubmit a new script everytime if we just want to run the training multiple times.  Pipelines allows us to keep a definition of one or more scripts in the cloud, which we can then invoke either via the SDK, CLI, or via a REST call. It also allows us to break down our training logic into reusable components that we can use in multiple different pipelines.

Azure ML Pipelines consists of a series of python scripts with a defined run order. Each step may run on separate compute and in separate environments. For example, you can use a small cluster for data preprocessing, and then a GPU-enabled compute instance to train your large deep learning models, without incurring excessive cost.

However, they can be annoying to debug since they (for some reason) can't run on your local computer, and need to be submitted to Azure ML.

Each PipelineStep is very similar to a ScriptRunConfigs, with some minor changes. Important to note, there is a slight difference in how Runs work in pipelines. The pipeline itself has it's own parent run, and each pipeline step is its own child run with its own run id. For convenience, it is nice to log metrics both in the child and the parent-run. Similarly, it is often better to register the model to the parent-run, rather than the child run, since it makes it easier to inspect the logs in the Studio. Get the parent run and replace the normal run.log with parent_run.log

For example
#+begin_src python
run.log('rmse', rmse)
run.parent.log('rmse', rmse)
#+end_src

Create a new orchestration script ml_pipelines/build_pipeline.py and copy the contents of the current run_train_script. Replace the script config definition in the build_pipeline file.
#+begin_src python
# ml_pipelines/build_pipeline.py (old)
from azureml.core import ScriptRunConfig
# ...
src = ScriptRunConfig('src', script='train.py', environment=environment)
run = experiment.submit(src)
run.wait_for_completion(show_output=True)
#+end_src

to a Python Script Step and a pipeline. If you defined some arguments earlier you can add them as well.
#+begin_src python
# ml_pipelines/build_pipeline.py (new)
run_config = RunConfiguration()
# Remember to set our favorite environment
run_config.environment = environment

train_step = PythonScriptStep(
    name="training_step",
    script_name="train.py",
    source_directory="src",
    compute_target=cpu_cluster,
    runconfig=run_config,
    allow_reuse=False,
    # arguments = [...]
)

pipeline = Pipeline(
    workspace=workspace, steps=[train_step], description="Model Training and Deployment"
)
pipeline.validate() # Make sure the pipeline is functioning

pipeline_name = <some-good-pipeline-name>
published_pipeline = pipeline.publish(pipeline_name)
print(published_pipeline.id)
#+end_src

#+begin_src python
python -m ml_pipelines.build_pipeline.py
#+end_src

Take note of the pipeline id which was printed, since it is needed to identify the pipeline in your workspace (you can also look in the Studio under the Pipeline page and see if a new pipeline was registered).

The pipeline object now exists as a callable object your workspace and can easily be executed whenever we see fit, regardless if the code change or not. We can then create a new orchestration script ~ml_pipelines/run_pipeline.py~ that will pipeline invoke the pipeline

#+begin_src python
# ml_pipelines/run_pipeline.py
from azureml.core import Experiment
from azureml.pipeline.core import PublishedPipeline

workspace = Workspace.from_config()
pipeline = PublishedPipeline.get(workspace, id=<pipeline-id>)
experiment = Experiment(workspace, <name-of-your-experiment>)
run = experiment.submit(pipeline)
status = run.wait_for_completion(show_output=True)
print(status) # Should say finished
#+end_src

Tips: If you just want to run the latest version of a pipeline and you lost the id you can just get the list of all pipeline and filter by name, and then select the first in that list.

#+begin_src python
pipelines = PublishedPipeline.list(workspace)
piplines = [p for p in pipelines where p.name == "<name-of-your-pipeline>"]
pipeline = pipelines[0]
#+end_src

* Pipeline Parameter
One problem with our current pipeline is that we have no means to change how we invoke it. Everytime we call it will just run the same set of steps, producing the same results (except if we uplaod a new version of the dataset). But what if we wanted to change the name of the model we produce, or download a different dataset. How could we do that without having to submit a new pipeline, which was the whole point of using pipelines in the first place.

Fortunently, pipelines have a utility called PipelineParameters, which is a way to dynamically change the arguments given the different script when invoked. And a PipelineStep takes a PipelineParameter as input, the parameter becomes part of the Pipeline Defintion, and can be set when the pipeline is invoked.

Say that you have a script that take a parameter named ~arg~
#+begin_src python
parser = argparse.ArgumentParser()
parser.add_argument('--arg', default=None)
args = parser.parse_args()
print(args.arg)
#...
#+end_src

A parameter is defined like this
#+begin_src  python
parameter = PipelineParameter('my-arg', default_value=1)
step = PythonScriptStep(name="step", script_name="step.py", arguments=['--arg', parameter], ...)

pipeline = Pipeline(ws, steps=[step])
pipeline.validate()
published_pipeline = pipeline.publish('my-pipeline')#+end_src
#+end_src

You can then change the PipelineParameter value like this
#+begin_src python
run = experiment.submit(published_pipeline, pipeline_parameters={'my-arg': 2})
status = run.wait_for_completion(show_output=True)
#+end_src
Or by using REST
#+begin_src python
import requests
response = requests.post(
    published_pipeline1.endpoint,
    json={"ExperimentName": "some-experiment",
        "ParameterAssignments": {"my-arg": 2}}
)
#+end_src

As a final excerice, modify you pipeline_build script to add pipeline arguments that can change the name of the model. Then, modify the run_pipeline script to take the model name as an optional parameter, which it then provides as a pipeline parameter when invoking the pipeline. This might seem quite basic, but it will be useful in the next day of the workshop, when we want to configure the pipeline to run certain checks given certain conditions.


* Optional Section
If you have come this far and still have some time left to kill, you can dive deeper into some more advanced pipeline topics

** Pipeline Endpoint
Having to keep track pipeline IDs quickly becomes annoying. Named Pipeline Endpoints offer a consistent endpoint that you can use to invoke your latest pipeline. You set your latest pipeline as the endpoint's default and simply call the endpoint as you would any other pipeline. You can then change the default pipeline of the endpoint whenever you build a new pipeline, and still invoke it the same way.

You can create/update a pipeline endpoint using a simple script, like this
#+begin_src python
# ml_pipelines/set_endpoint.py
import argparse
from azureml.pipeline.core import PublishedPipeline, PipelineEndpoint
from azureml.core import Workspace
from ml_pipelines.utils import EnvironmentVariables

ws = Workspace.from_config()
env_vars = EnvironmentVariables()

parser = argparse.ArgumentParser()
parser.add_argument("--pipeline-id", required=True, help="Published Pipeline to invoke")
arguments = parser.parse_args()

published_pipeline = PublishedPipeline.get(ws, id=arguments.pipeline_id)
try:
    pipeline_endpoint = PipelineEndpoint.get(ws, name=env_vars.pipeline_endpoint_name)
    pipeline_endpoint.add_default(published_pipeline)
except Exception:
    pipeline_endpoint = PipelineEndpoint.publish(
        workspace=ws,
        name=env_vars.pipeline_endpoint_name,
        pipeline=published_pipeline,
        description="Pipeline Endpoint for Departure Prediction",
    )
#+end_src

then you change your run script to use the endpoint instead of the pipeline
#+begin_src python
from azureml.pipeline.core import  PipelineEndpoint
# ..
published_pipeline = PipelineEndpoint.get(ws, name=env_vars.pipeline_endpoint_name)
experiment = Experiment(ws, env_vars.experiment_name)
run = experiment.submit(published_pipeline, pipeline_parameters=pipeline_parameters)
status = run.wait_for_completion(show_output=True)
print(status)
#+end_src

** More Advanced Pipelines
The current pipeline is embarrassingly simple, merely consisting of a single step. As our project matures, we should start to break up the training script into smaller sub-scripts, such that they can be reused in other pipelines or by other members of the project. Maybe one script to check if we have new data to train on, one script to clean the data, and so forth.

If you want an example of how your can create more (read overly) complex pipelines with different steps, you can look at the ~complex_pipeline~ in the example repo. You find the script definitions [[https://github.com/lukas-lundmark/mlops-example/tree/main/src/complex_pipeline][here]], and the various orchestration scripts [[https://github.com/lukas-lundmark/mlops-example/tree/main/ml_pipelines/complex_pipeline][here]].
