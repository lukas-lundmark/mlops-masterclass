#+title: Azure ML Day 1
#+author: luklun

* Getting Started
If you haven't done so already, set up your [[./setup-python.org][Python Environment]] and then the [[./setup-azure.org][Azure CLI]].

* Interactive Experiment Tracking
We are gonna start with getting familiar with Azure ML and how it can be used to track our interactive notebook experiments. In a normal DS project, this experiment phase will take up the majority of the time. In this workshop, however, we will speed through this phase by starting from a preexisting notebook. Create a new directory called notebooks or experiments. Copy the notebook *notebooks/Exploratory Data Analysis Local.ipynb* from the example repo and place it in a notebook folder in your repo.

** Starting a Jupyter Notebook
Before you start the notebook it can be a good idea to login to your [[./setup-azure.org][Azure CLI]]. Also, make sure you have activated your python environment. Then run

Start a Jupyter Notebook Server from the root of your repo's root directory and try to run through the notebook one time. You will need to download the data from the [[https://drive.google.com/drive/u/0/folders/1BuHRP-4f8Ai4j8nqwLYcB1KZSJIy7mQz][Google Drive]] and place it in a folder called data to run the experiment.

** Access your workspace
After you have made sure that the experiments work locally, the next step is to connect to the cloud. On the page for your Azure ML workspace in the Azure Portal (not the Studio), download the workspace config file and add it to the root folder. This configuration file identifies your Azure ML workspace and if you are in an environment with access to that workspace, i.e., you are logged in via your Azure CLI, you have access to all of the workspace's resources.

In your notebook, import the Azure Machine Learning package. If you initialized the environment from the conda yaml file it should already be installed. You can import all the classes we will use right away

#+begin_src python
from azureml.core import Workspace, Experiment, Dataset, Datastore, Run, Model
from azureml.exceptions import UserErrorException
#+end_src


Access your workspace from the config by instantiating a workspace object.

#+begin_src python
ws = Workspace.from_config()
#+end_src

If you haven't logged in from your Azure CLI, you will be prompted to do so in your notebook.

** Register a dataset
One of the most basic utilities AML offers is the ability to store and version your datasets. Import your data as a pandas data frame. Clean it if you like. Then, we want to register the dataset

#+begin_src python
import pandas as pd
df = pd.read_csv(...)
#+end_src

You can register and upload a dataset the following way
#+begin_src python
datastore = Datastore.get_default(ws)
dataset = Dataset.Tabular.register_pandas_dataframe(
    df,
    datastore,
    show_progress=True,
    name='my-dataset',
    description='This is some dataset'
)
#+end_src
The datastore is a blob storage account where the data you upload will be stored. Each Azure ML workspace is given a new blob storage account by default which is most often used when uploading new data.

The datasets object will not contain the actual data. Instead, it contains meta information, such as id, name, and version, as well as the storage location of the data. If you want to use the data, you need to materialize it somehow, for example by downloading it as a data frame.

#+begin_src python
df = dataset.to_pandas_dataframe()
#+end_src

It's a good idea to check if the dataset already exists and download it if it does. Otherwise, you will keep adding a new version of the dataset every time your re-run your notebook.

A check like this is commonly used
#+begin_src python
try:
    dataset = Dataset.get_by_name(ws, name="my-dataset")
except UserErrorException as ue:
    datastore = Datastore.get_default(ws)
    dataset = Dataset.Tabular.register_pandas_dataframe(
        #...
#+end_src

If you want a full example of how this can be done, look at this example [[https://github.com/lukas-lundmark/mlops-example/blob/main/notebooks/Exploratory%20Data%20Analysis%20with%20Tracking.ipynb][repo]].

For the workshops, since we have two datasets - one training and one test - you should create two datasets that you upload to Azure ML.

After this, you should be able to restart your notebook completely and have the data downloaded from Azure ML automatically. This enables anyone else who has access to your workspace to run your notebooks, without having to share the data or having to add the data to the code repository.

** Run Interactive Experiments with Experiment Tracking
Now that the data works, we should move down to the training code in the notebook. We want to define an experiment that can store our runs. Set a suitable experiment name, e.g., diamond-regression-experiment-interactive

#+begin_src python
experiment = Experiment(ws, 'workshop-experiment')
#+end_src

Then, instatiate an interactive run.
#+begin_src python
run = experiment.start_logging(snapshot_directory='.')
#+end_src

If you go to Azure ML Studio you should now be able to see that the experiment has been created and that it has a running run.

Change the code to log the metric using the run.log to log metrics
#+begin_src python
run.log('r2', r2_value)
run.log('rmse', rmse_value)
#+end_src

Finish the interactive run with
#+begin_src python
run.complete()
#+end_src

#+attr_html: :style margin-left: auto; margin-right: auto; :width 450px
[[./figures/run-overview.png]]

Check on your workspace that the experiment run was completed and that the results of your run were logged in the run. If it's marked as still running it means you forgot to run the src_python{run.complete(). If you want, you can manually complete or cancel the run from within the Studio.

#+attr_html: :style margin-left: auto; margin-right: auto; :width 450px
[[./figures/azure-ml-code.png]]

Check the log and code section of the run. You can see that each run has a local copy of your source directory. This is what you specified with the *snapshot_directory parameter*. This is a good way for others to see what code was used to generate a run. Similarly, the run also contains information regarding your git repository, showing which commit was used to run the experiment and if the branch was dirty when the code ran.

** Register a model in the Model Registry and log metrics
Registering a model is straightforward. We just need to save the model locally in a format we can load later, joblib is usually the standard for scikit-learn models. Joblib is a modified version of python's standard serializing library pickle which is better optimized for binary data

Something like this should already exist in your notebook
#+begin_src python
from pathlib import Path
import joblib

path = Path("outputs", "model.pkl")
path.parent.mkdir(exist_ok=True)
joblib.dump(model, filename=str(path))
#+end_src

Then, we need to upload the model to somewhere Azure ML can access it. We can use the run for this. Uploading a file to the run will result in it being available in the run's history and its logs

#+begin_src python
run.upload_file(str(path.name), path_or_stream=str(path))
#+end_src

Then, you can register the model to your workspace.
#+begin_src python
run.register_model(model_name="my-regressor", model_path=str(path.name), description="blah blah")
#+end_src

However, it is good practice to add as much meta information as possible to the model. Since we register the model to the run, there will always be a link to the run and the model, and you can always get access to the log information of the run via the model. However, to make things easier when inspecting our model registry, we want to add things like datasets used for training and testing, metrics, which framework the model used, etc.

#+begin_src python
run.register_model(
    model_name="my-regresssor",
    model_path=str(path.name),
    description="Very good regression model",
    model_framework="ScikitLearn",
    datasets=[("training dataset", train_dataset), ("test dataset", test_dataset)],
    tags={"rmse": rmse, "r2": r2}
)
#+end_src

The final thing we want is to make sure that we only register models that improve on earlier results. It is still okay to upload the model to the run, but it's a bad idea to clutter your registry with subpar models.

By registering the metrics in the model tags, as we did, we make makes things easier for us.

You can get all models with the same name by using
#+begin_src python
all_models = Model.list(ws, name="my-regressor")
#+end_src

Then, you can inspect the tags of these models for the metrics and register the model if it's better, e.g.,
#+begin_src python
if all(rmse <  float(model.tags.get("rmse", np.inf)) for model in all_models):
    run.register_model(..., tags={"rmse": rmse, ...})
#+end_src

Or, if you start using a new model name, you can enforce this rule of only registering the best model from the beginning. Then, you know that the latest model registered is always the best, and only needs to compare to that. You can then get the latest model and compare it with your current result

#+begin_src python
try:
    latest_model = Model(ws, name="my-regressor")
    r2 = latest_model.tags.get("r2", np.inf)
except WebserviceException as e:
    # ...
#+end_src
