#+title:
#+author: luklun
#+date: 2022-05-14

* Deploying Models
You should now have a remarkable POC model that is garuanteed to provide value to the business, we now want to hand it over to operations and have it deployed as a webservice that our.

* Deploying Machine Learning Models
There are three general approaches to serving of machine learning models.

1. Online
   The model is exposed to users and programmers as an API running as a hosted webservice. Needs to have little delay and be able to scale quickly.

2. Batch
   The model can be exposed as an API. Will run prediction on large batches of data, often in cloud storage, and write the result. Startup delays are tolerated, meaning that compute costs can be reduced.

3. Edge
   Models are deployed to an edge device, like a Rasberry Pie, a sensor.

This workshop will focus on Online deployment, since it is the easiest to setup and test.

* Deploying in Azure ML
Azure ML provides a variety of different methods to deploy a model.

1. Model Packaging
   Azure ML builds a containerized REST API service for your model which you can run anywhere.

2. Webservice
   Essentially Model Packaging but is managed by Azure ML. It reates things like key authentication.  Webserices can run locally (mostly for debugging) but is meant to run using Azure ML compute.

3. Batch Inference Pipeline
   Same training pipelines, however, it has a special configuration that allows it to run certain steps in parallel.

4. Online Endpoint
   A feature in public preview (i.e., not fully released yet). Endpoints are a more advanced version of webservices. It is essentially a Wrapper around Kubernetes, where you may have multiple deployments of your webservice running and you can configure how much traffic each version should get. This allows you to shift traffic gradually from older stable version to newer versions, so called blue-green deployment.

5. Batch Online Endpoint
   Similar to online endpoint, but for batch inference. More latency, but less expensive to run.

In this workshop, we will rely mostly on Model Packaging and Web Services.

* Microservices and Webservices
Models are often deployed as microservices, i.e., a small, self-contained web services.
Often one uses a microservice library, such as Flask, or FastAPI, to expose a REST API for the model.
A very basic Machine Learning microservice can look something like this (this is just an example)

#+begin_src python
# Initialize the Flask webservice
import joblib
from flask import Flask, requests, jsonify
app = Flask(__name__)

# Initialize the model from disk
def init():
    global model # Add the model variable to the global namespace
    model = joblib.load("path-to-model.joblib")

# Create a route in the API that runs the models and returns predictions
@app.route("/score", methods=['POST'])
def run():
    json_data = requests.json
    inputs = json_data['inputs']
    predictions = model.predict(inputs)
    return jsonify({"predictions": predictions})

if __name__ == "__main__":
    init()
    app.run()
#+end_src

This can then be called as any other API
#+begin_src python
import requests
response = requests.post("https://127.0.0.1:5000:/score", data={"inputs": [0.1, 0,2]})
print(response.json)
#+end_src

These microservices are then usually deployed as docker containers containing the necessary dependencies for running the service.

* Microservices using Azure ML
Most of Azure ML webservice deployments relies on an almost identical setup to the previously mentioned flask approach. They define the interface of the service using a simple 'scoring' script and define the runtime environment using something called AML environments.

In the next session we will go through some theory in order to give you a better understanding of how AML manages services. You don't have to program anything until the next main section called packaging web services, as the code snippets are mostly here as examples.

** Inference Config
The two ncessary components mentioned above are defined using an InferenceConfig object. It only needs three arguments. The entry script (aka scoring script), the root directory of the source code that contains the scoring script (this allows us to import other files in that directory in our scoring script), and the environment.
#+begin_src python
from azureml.core.model import InferenceConfig
# ...
inference_config = InferenceConfig(
    source_directory="<directory-with-the-source-code>",
    entry_script="<relative-path-to-script-from-src-dir>.py",
    environment=environment,
)
#+end_src

** Scoring Scripts
Scoring scripts only defines two functions : *init*, which takes no arguments and only runs a single time when the container is initialized. This method should be used to initialize the model by e.g., load it from the model registry. The second, *run*, takes as input the data from a POST call and returns the models predictions.

Here is an example of a simple scoring script
#+begin_src python
import os
import json

def init():
    global model
    # This environment variable points to a folder that will contain your model file
    model_path = os.path.join(os.getenv("AZUREML_MODEL_DIR"), "name-of-my-model")
    model = joblib.load(model_path)

def run(raw_data):
    records = json.loads(raw_data)
    df = pd.DataFrame(records)
    df["predicted_price"] = model.predict(df)
    return json.dumps(df.to_dict(orient="records"))
#+end_src


*** Specifying and Accessing Models
There are multiple ways of loading the model from within a scoring script. When you define a package or a deployment (more on that later), you should specify a Model object that will be used to download the model files from your workspace and put them in a folder in the container running your scoring script. Something like this

#+begin_src python
# Get the model definition
model = Model(ws, name='my-awesome-model')
# Package model as a service
package = Model.package(
    workspace,
    models=[model],
    # ... More arguments here
)

#+end_src

#+RESULTS:

By default, the model is downloaded to a randomly named directory, and that directory is defined in the environment variable ~AZURE_MODEL_DIR~ by Azure ML (If you don't specify a model, this variable will not be set). If you only specify one model (you can provide as many as you want), you can just load the model by loading whatever is in that directory

#+begin_src python
from pathlib import Path

model_path = next(Path(os.getenv("AZURE_MODEL_DIR")).glob('*'))
model = joblib.load(model_path)
#+end_src

Another way is to use the name of the model
#+begin_src python
model_path = Model.get_model_path('my-awesome-model')
#+end_src
this is useful if you specify multiple models, but it requires the script to know the name of the models which makes it less flexible


*** Imports from within the Scoring script
When the Webservice invokes your scoring script it will do so from the source directory you specified in the inference configuration. The entire source directory is also copied to the service. This means that you can import functions from other files in your source directory. For example, if you have function called *clean_data* in ~src/data/prepare.py~, and a folder structure like this

#+begin_src
src/
    score.py
    data/
        prepare.py
#+end_src
where src is the ~source_directory~, and score is the scoring script
you can import it as follows in your scoring script

#+begin_src python
# src/score.py
from data.prepare import clean_data
# ...
#+end_src

this is a convinient way further split up your training and scoring functionality.

** Environments
Azure ML's Environments are basic wrappers around Docker with some sane defaults. We will use the same conda definition file that initialized our local environment to create an Azure ML environment. We then let AML build the docker image from it's default ubuntu image and instantiate the conda environment within. (Later on, we will build our own images using a similar approach. But for now, we let Azure ML do the heavy lifting.)

Since we already have a conda definition file, we can use that to define the environment. Similarly to when we created datasets, we first check if the environment exists in the repo, and otherwise we create it

#+begin_src python
conda_file = 'environment_setup/ci_dependencies.yml'
environment_name = 'my-environment'

try:
    env = Environment.get(ws, name=environment_name)
except Exception:
    assert env_vars.environment_file is not None
    print("No Environment Found")
    env = Environment.from_conda_specification(
        name=environment_name, file_path=conda_file
    )
    # You need to manually register it for it to be available later
    env.register(ws)
#+end_src
This environment will be used to run almost all future scripts so its useful if we can access it in all of our orchestration scripts. A good idea is to extract such functions into a common utility module or folder. For your conveniance, an example of this functino have already been defined in the ~ml_pipelines/utils.py~ file. Later on, we will need to add other utility functions for creating resources here.

*** Arguments and environment variables
Let's go on a little tangent. Environment variables are a convinient way to control your application without having to change how you call it. You may for example have your own name for the model when debugging locally, vs. when running it in production.

However, loading environment variables in Python can easily get convoluted. Although one can access any environment variable using the src_python{os.environ} dictionary, it quickly becomes unclear for others reading your code what variables needs to be set for your code to run and what their purpose are.

A more well-structured method is to use a combination of python-classes and the package python-dotenv to manage variables. The src_python{load_dotenv}  function in the module dotenv will look for a file named .env in your current working directory and load the environment variable definitions in it. If it can't find any, it will do nothing. You can create a .env file with content like, setting those environment variables you want

#+begin_src bash
# ./.env
VARIABLE_NAME1="variable-value"
VARIABLE_NAME2="variable-value2"
#+end_src

See the [[https://github.com/lukas-lundmark/mlops-example/blob/main/.env.example][.env.example]] in the example repo

By wrapping a call to load_dotenv and the subsequent loading of the variables within a class creates a single point of truth where all relevant variables are defined (and optionally documented). It can look like this:

#+begin_src python
# ml_pipelines/utils.py
import os
from dataclasses import dataclass
from dotenv import load_dotenv

from typing import Optional

@dataclass
class EnvironmentVariables:
    load_dotenv()
    model_name: Optional[str] = os.environ.get("MODEL_NAME", "default-model-name")
    experiment_name: Optional[str] = os.environ.get("EXPERIMENT_NAME", "default-experiment-name")
    ...

#+end_src

There already exists a small outline for this class in the ~ml_pipelines~ folder in the template repo that you can start using. You can start to add environment variables to this as you need more and more configurations

Variables that might good to define are:
- model name
- experiment name
- environment name
- service name
- aks-cluster name
- script directory
- scoring file
- train and test dataset names
- conda file

If you want some inspiration you can see the example class in the [[https://github.com/lukas-lundmark/mlops-example/blob/main/ml_pipelines/utils.py][example repository]] or the [[https://github.com/microsoft/MLOpsPython/blob/master/ml_service/util/env_variables.py][Microsoft's Azure ML MLOps directory]].

You can then use the ~EnvironmentVariables~ class in your orchestration script to get quick access to the relevant variables.
#+begin_src python
from ml_pipelines.utils import EnvironmentVariables

# Load relevant environment variables
env_vars = EnvironmentVariables()
# ...
model = Model(workspace, name=env_vars.model_name)
experiment = Experiment(workspace, name=env_vars.experiment_name)
#+end_src

Later, when we run our scripts in a CI/CD environment, we can just set these variables in the environment definition.


* Packaging your webservice
There are a various options for how to deploy a service: WebServices, Online Endpoints, Batch Endpoints, just to name a few. We will mostly focus on WebServices in this workshop since they offer a good level of abstraction. However, first, we will try to package our model as a service in a docker container. Packaging a model is similar to deploying a WebService, except we can create and run the resulting Docker image manually, meaning we can inspect the generated docker and service code, which is good for both understanding what is going on under the hood and for debugging.

The template repo contains a dummy scoring script that will just respond with the same input it was given in ~src/service/score.py~. There is also some unfinished packaging code for packaging the model in ~ml_pipelines/deploy/package_service.py~ which helps you creates a local docker image, but doesn't specify a model. Our goal is to make the service respond with the predictions of the model we trained earlier. In order to do this we need to do two things, finish the scoring script such that it loads and responds to requests with the model, and specify which model to give to the service in the orchestration script.

The code in the orchestration script looks something like this. This code just specify the deployment
#+begin_src python
# ml_pipeline/deploy/package_service.py
from azureml.core import Workspace, Model
from azureml.core.model import InferenceConfig
from ml_pipelines.utils import EnvironmentVariables, get_environment
# ...

workspace = Workspace.from_config()
env_vars = EnvironmentVariables()
environment = get_environment(workspace, env_vars)
inference_config = InferenceConfig(
    entry_script=env_vars.scoring_file,
    source_directory=env_vars.scoring_dir,
    environment=environment,
)
# Will return the latest model version
#+end_src

We then have some code that packages the model and creates a webservice in a Dockerfiles that is saved to a local folder in imagefiles
#+begin_src python
package = Model.package(
    workspace,
    models=[], # <- Here we want to add list containing a model, i.e. [model]
    inference_config=inference_config,
    generate_dockerfile=True
)
package.wait_for_creation(show_output=True)
package.save("./imagefiles")
#+end_src

Currently, the service doesn't specify a model to use. Later we want to download the model definition and give it to the service. E.g.,
#+begin_src python
model = Model(workspace, name=env_vars.model_name, version=None)
# ...
package = Model.package(
    workspace,
    models=[model],
    inference_config=inference_config,
    generate_dockerfile=True
)
#+end_src

You can run the script as is the first time and follow the instructions. Run it as a python module from the repo's root directory so that the src_python{from ml_pipelines.utils ...} imports works
#+begin_src bash
python -m ml_pipelines.deploy.package_service
#+end_src
A tip is to do this for all the scripts in ml_pipelines.

The first time it might take a while to create the environment, but later builds should more or less be instant. If you followed the instructions you should have a webservice running locally using a docker container that you built yourself. Packaging the model like this a good method for when you want to debug your scoring script without having to push new images to your container registry every time.

You goal now should be to configure the scoring script and the package script, such that you download your registered model and make it take requests. You can also look at the scr/deployment/score.py in the example repo for a more complete example of a scoring script if you feel lost.

What you need to do: Update the orchestration script by giving your model to the ~Model.package~ call. Update the scoring script to take

Note: at some points you might be asked to login to your azure container registry in the azure CLI.
#+begin_src bash
az acr login --name <name-of-your-azure-container-registry>
#+end_src
You can find the name of your container registry by looking in the Azure Portal. It is the container registry you created when you created your workspace at the beginning.

** Testing your service
In order to test your service we provide a basic python script ~send_request.py~ in the template directory.

This will command will send 10 example records to the deployment and print the response
#+begin_src bash
python send_request.py --url <target-uri>/score [--key <key>] --file data/diamonds-test.csv --n 10
#+end_src

The URL is the URI of the webservice, which should be http://localhost:6789/score for the packaged service. The key flag can be ignored for now, since we are running locally. The file flag points to the test data we have downloaded and will send n records from it

The script sends the data as a list of records (in json format) src_json{[{'x': 1, 'y': 2, ...}...]} which you can convert to a dataframe as follows

#+begin_src python
raw_json_data = "[{'x': 1, 'y': 2, ...}...]"
records = json.loads(raw_json_data)
df = pd.DataFrame(records)
# Sometimes you may want to set the type of the columns
df.astype({'x': float, 'cut': object, ...})
#+end_src

* Azure ML Local Webservice
Having a local docker image is a neat way to see what is going on under the hood. However, Azure ML offers a further abstraction in the form of WebServices. A webservice allows you to automatically deploy the docker we built in the previous step to a compute resource of your choice. It also handles things like authentication and load balancing.

In order to test how a webservice performs, it is a good idea to perform a local deployment first. Instead of packaging the model - like we did in the previous step - we *deploy* the model. The only difference from before is that we also provide a deployment configuration and a name for the service.

The deployment configuration defines what kind of compute the model should run on. Since we want to run it locally, we can use the LocalWebservice class to create a local deployment configuration.

#+begin_src python
from azureml.core.webservice import LocalWebservice
# ...
deployment_config = LocalWebservice.deploy_configuration(port=6789)
#+end_src

Then, we can create a deployment as the following
#+begin_src python
service = Model.deploy(
    workspace=workspace,
    name="<name-of-service>",
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config,
    overwrite=True,
    deployment_target=None
)
service.wait_for_deployment(show_output=True)
print(service.scoring_uri)
#+end_src

Create a new script called ~deploy_service.py~ in the same folder as the package script. Copy most of the contents from the  ~ml_pipeline/deploy/package_service.py~ down to the ~package.wait_for_creation()~ call. Then, replace the packaging with the code above. This should give you a webservice running locally that you can

This is very similar to before, but now Azure ML is responsible for the building and managing the docker container we previously built ourselves. The next step is to run this container somewhere other than our own computer.

* Deploying to Azure ML Compute
If we want to make the service available to everyone, we need to deploy it to the cloud. Defining a small kubernetes cluster is fairly easy in Azure ML. If you have a free subscriptions, your vCPU quota is usually quite small, so we should limit ourselves to using a single node. Standard production clusters require a minimum of three nodes, so you should set the cluster purpose to DEV_TEST.

Something like this should do the trick.
#+begin_src python
from azureml.core.compute import ComputeTarget, AksCompute
from azureml.core.compute_target import ComputeTargetException
# ...

inference_cluster_name = "my-aks"
try:
    aks_target = AksCompute(workspace, name=inference_cluster_name)
except ComputeTargetException:
    provisioning_config = AksCompute.provisioning_configuration(
        vm_size='Standard_D2as_v4', # The smallest size
        agent_count = 1,
        cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST # Needed for having less than three nodes
    )
    aks_target = ComputeTarget.create(
        workspace = workspace,
        name = inference_cluster_name,
        provisioning_configuration = provisioning_config
    )
    aks_target.wait_for_completion(show_output = True)
#+end_src
A tip is to extract this as a function in the ~utils.py~ and use the EnvironmentVariable to configure the parameters, such as the cluster name.

We also need to create a new deployment configuration for our AKS cluster. We should update our ~deploy_service.py~ to take an new deployment config and the new compute target
#+begin_src python
from azureml.core.webservice import AksWebservice
# ...
model = Model(workspace, name=env_vars.model_name)
deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)

service = Model.deploy(
    workspace=workspace,
    name=env_vars.service_name,
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config,
    overwrite=True,
    deployment_target=aks_target
)
service.wait_for_deployment(show_output=True)
print('uri', service.scoring_uri)
print('key', service.get_keys()[0]) # Your service is per default protected by key authentication
#+end_src

Running this script can take up to 10 minutes, with most of the time being spent setting up the compute cluster. However, since we didn't change the scoring or environment, that build should be almost instant.

Use the uri and key to send some test request to your new service. If you lost them you can always see them in the studio. Test that your service works using our neat testing script

* Final Thougths
So far, we have created a very basic POC and we have already deployed it as a webservice running in a mock production environment. Your little project can now be considered as being MLOps Level 0. The next step is now to move towards MLOps Level 1, by making our training automated. In the step after that, we will move towards running training in GitHub.
