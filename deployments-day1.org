#+title:
#+author: luklun
#+date: 2022-05-14

* Deploying Models
You should now have a remarkable POC model that is garuanteed to provide value to the business, we now want to hand it over to operations and have it deployed as a webservice that our.

* Deploying Machine Learning Models
There are three general approaches to serving of machine learning models.

1. Online
   The model is exposed to users and programmers as an API running as a hosted webservice. Needs to have little delay and be able to scale quickly.

2. Batch
   The model can be exposed as an API. Will run prediction on large batches of data, often in cloud storage, and write the result. Startup delays are tolerated, meaning that compute costs can be reduced.

3. Edge
   Models are deployed to an edge device, like a Rasberry Pie, a sensor.

This workshop will focus on Online deployment, since it is the easiest to setup and test.

* Deploying in Azure ML
Azure ML provides a variety of different methods to deploy a model.

1. Model Packaging
   Azure ML builds a containerized REST API service for your model which you can run anywhere.

2. Webservice
   Essentially Model Packaging but is managed by Azure ML. It reates things like key authentication.  Webserices can run locally (mostly for debugging) but is meant to run using Azure ML compute.

3. Batch Inference Pipeline
   Same training pipelines, however, it has a special configuration that allows it to run certain steps in parallel.

4. Online Endpoint
   A feature in public preview (i.e., not fully released yet). Endpoints are a more advanced version of webservices. It is essentially a Wrapper around Kubernetes, where you may have multiple deployments of your webservice running and you can configure how much traffic each version should get. This allows you to shift traffic gradually from older stable version to newer versions, so called blue-green deployment.

5. Batch Online Endpoint
   Similar to online endpoint, but for batch inference. More latency, but less expensive to run.

In this workshop, we will rely mostly on Model Packaging and Web Services.

* Microservices and Webservices
Models are often deployed as microservices, i.e., a small, self-contained web services.
Often one uses a microservice library, such as Flask, or FastAPI, to expose a REST API for the model.
A very basic Machine Learning microservice can look something like this (this is just an example)

#+begin_src python
# Initialize the Flask webservice
import joblib
from flask import Flask, requests, jsonify
app = Flask(__name__)

# Initialize the model from disk
def init():
    global model # Add the model variable to the global namespace
    model = joblib.load("path-to-model.joblib")

# Create a route in the API that runs the models and returns predictions
@app.route("/score", methods=['POST'])
def run():
    json_data = requests.json
    inputs = json_data['inputs']
    predictions = model.predict(inputs)
    return jsonify({"predictions": predictions})

if __name__ == "__main__":
    init()
    app.run()
#+end_src

This can then be called as any other API
#+begin_src python
import requests
response = requests.post("https://127.0.0.1:5000:/score", data={"inputs": [0.1, 0,2]})
print(response.json)
#+end_src

These microservices are then usually deployed as docker containers containing the necessary dependencies for running the service.

* Microservices using Azure ML
Most of Azure ML webservice deployments relies on an almost identical setup to the previously mentioned flask approach. They define the interface of the service using a simple 'scoring' script and define the runtime environment using something called AML environments.

In the next session we will go through some theory in order to give you a better understanding of how AML manages services. You don't have to program anything until the next main section called packaging web services, as the code snippets are mostly here as examples.

** Scoring Scripts
Scoring scripts only defines two functions : *init*, which takes no arguments and only runs a single time when the container is initialized. This method should be used to initialize the model by e.g., load it from the model registry. The second, *run*, takes as input the data from a POST call and returns the models predictions.

Here is an example of a simple scoring script
#+begin_src python
import os
import json

def init():
    global model
    # This environment variable points to a folder that will contain your model file
    model_path = os.path.join(os.getenv("AZUREML_MODEL_DIR"), "name-of-my-model")
    model = joblib.load(model_path)

def run(raw_data):
    records = json.loads(raw_data)
    df = pd.DataFrame(records)
    df["predicted_price"] = model.predict(df)
    return json.dumps(df.to_dict(orient="records"))
#+end_src

*** Imports from within the Scoring script
When the Webservice invokes your scoring script it will do so from the source directory you specified in the inference configuration. The entire source directory is also copied to the service. This means that you can import functions from other files in your source directory. For example, if you have function called *clean_data* in ~src/data/prepare.py~ you can import it as follows in your scoring script

#+begin_src python
# src/score.py
from data.prepare import clean_data
# ...
#+end_src

this is a convinient way further split up your training and scoring functionality.

** Environments
Azure ML's Environments are basic wrappers around Docker with some sane defaults. We will use the same conda definition file that initialized our local environment to create an Azure ML environment. We then let AML build the docker image from it's default ubuntu image and instantiate the conda environment within. (Later on, we will build our own images using a similar approach. But for now, we let Azure ML do the heavy lifting.)

Since we already have a conda definition file, we can easily create the environment like the following. Here we use a very commong access pattern in Azure ML where we first try to download a resource, and creates it if we couldn't locate it

#+begin_src python
conda_file = 'environment_setup/ci_dependencies.yml'
environment_name = 'my-environment'

try:
    env = Environment.get(ws, name=environment_name)
except Exception:
    assert env_vars.environment_file is not None
    env = Environment.from_conda_specification(
        name=environment_name, file_path=conda_file
    )
#+end_src

We will reference this environment alot in other scripts so it is a good idea to extract it as a function somwhere other scripts can access it. It might also be a good idea to let it take an EnvironmentVariables argument such that we can easily configure environment name using an .env file (see next section). For your conveniance, a prebuilt version have already been defined in the ~ml_pipelines/utils.py~ file. But we will need to add many other such functions later on to create other resources.

*** Arguments and environment variables
Let's go on a little tangent. Environment variables are a convinient way to control your application without having to change how you call it. You may for example have your own name for the model when debugging locally, vs. when running it in production.

However, loading environment variables in Python can easily get convoluted. Although one can access any environment variable using the src_python{os.environ} dictionary, it quickly becomes unclear for others reading your code what variables needs to be set for your code to run and what their purpose are.

A more well-structured method is to use a combination of python-classes and the package python-dotenv to manage variables. The src_python{load_dotenv}  function in the module dotenv will look for a file named .env in your current working directory and load the environment variable definitions in it. If it can't find any, it will do nothing. You can create a .env file with content like, setting those environment variables you want

#+begin_src bash
# ./.env
VARIABLE_NAME1="variable-value"
VARIABLE_NAME2="variable-value2"
#+end_src

See the [[https://github.com/lukas-lundmark/mlops-example/blob/main/.env.example][.env.example]] in the example repo

By wrapping a call to load_dotenv and the subsequent loading of the variables within a class creates a single point of truth where all relevant variables are defined (and optionally documented). It can look like this:

#+begin_src python
# ml_pipelines/utils.py
import os
from dataclasses import dataclass
from dotenv import load_dotenv

from typing import Optional

@dataclass
class EnvironmentVariables:
    load_dotenv()
    model_name: Optional[str] = os.environ.get("MODEL_NAME", "default-model-name")
    experiment_name: Optional[str] = os.environ.get("EXPERIMENT_NAME", "default-experiment-name")
    ...

#+end_src

There already exists a small outline for this class in the ~ml_pipelines~ folder in the template repo that you can start using. You can start to add environment variables to this as you need more and more configurations

Variables that might good to define are:
- model name
- experiment name
- environment name
- service name
- aks-cluster name
- script directory
- scoring file
- train and test dataset names
- conda file

If you want some inspiration you can see the example class in the [[https://github.com/lukas-lundmark/mlops-example/blob/main/ml_pipelines/utils.py][example repository]] or the [[https://github.com/microsoft/MLOpsPython/blob/master/ml_service/util/env_variables.py][Microsoft's Azure ML MLOps directory]].

You can then use the ~EnvironmentVariables~ class in your orchestration script to get quick access to the relevant variables.
#+begin_src python
from ml_pipelines.utils import EnvironmentVariables

# Load relevant environment variables
env_vars = EnvironmentVariables()
# ...
model = Model(workspace, name=env_vars.model_name)
experiment = Experiment(workspace, name=env_vars.experiment_name)
#+end_src

Later, when we run our scripts in a CI/CD environment, we can just set these variables in the environment definition.


* Packaging your webservice
The template repo contains a dummy scoring script that will just respond with the same input it was given. Our goal is to convert the

and some simple code for


So, we should now have a scoring script and an environment. We then create an inference config from this information, and then find the latest version of our model.

#+begin_src python
# ml_pipeline/build_service.py
from azureml.core import Workspace, Model
from azureml.core.model import InferenceConfig
from ml_pipelines.utils import EnvironmentVariables, get_environment
# ...

workspace = Workspace.from_config()
env_vars = EnvironmentVariables()
environment = get_environment(workspace, env_vars)
inference_config = InferenceConfig(
    entry_script=env_vars.scoring_file,
    source_directory=env_vars.scoring_dir,
    environment=environment,
)
# Will return the latest model version
model = Model(workspace, name=env_vars.model_name, version=None)
#+end_src

We can then ask AML to package our model as a docker container for us and save the Dockerfile locally.
#+begin_src python
package = Model.package(
    workspace,
    models=[model],
    inference_config=inference_config,
    generate_dockerfile=True
)
package.wait_for_creation(show_output=True)
package.save("./imagefiles")
#+end_src

To help you get started, there already exists basic packaging script in ~ml_pipelines/deploy/package_service.py~ that will package this dummy scoring script for you (without an ml model). Run it and follow the instructions. The first time it might take a while to create the environment, but later builds should more or less be instant. If you followed the instructions you should have a webservice running locally in a docker that you built yourself. This a good method for when you want to debug your scoring script wihtout having to push new images to your container registry everyt time.

You goal now should be to configure the scoring script and the package script, such that you download your registered model and make it take requests. You can also look at the scr/deployment/score.py in the example repo for a more complete example of a scoring script if you feel lost.

Note: you might be asked to login to your azure container registry in the azure CLI.
#+begin_src bash
az acr login --name <name-of-your-azure-container-registry>
#+end_src

** Testing your service
In order to test your service we provide a basic python script ~send_request.py~ in the template directory.

This will command will send 10 example records to the deployment and print the response
#+begin_src bash
python send_request.py --url <target-uri>/score [--key <key>] --file data/diamond-test.csv -n 10
#+end_src

The script sends the data as a list of records (in json format) src_json{[{'x': 1, 'y': 2, ...}...]} which you can convert to a dataframe as follows

#+begin_src python
raw_json_data = "[{'x': 1, 'y': 2, ...}...]"
records = json.loads(raw_json_data)
df = pd.DataFrame(records)
# Sometimes you may want to set the type of the columns
df.astype({'x': float, 'cut': object, ...})
#+end_src

* Azure ML Local Webservice
Having a local docker image is a neat way to see what is going on under the hood. However, Azure ML offers a further abstraction in the form of Local WebServices. A webservice allows you to automatically deploy the docker we built in the previous step to a compute resource of your choice. It also handles things like authentication and load balancing.

In order to test how a webservice performs, it is a good idea to perform a local deployment first. Instead of packaging the model, like we did in the previous step, we deploy the model. The only difference from before is that we now also provide a deployment configuration and a name for the service.

The deployment configuration defines what kind of compute the model should run on. Since we want to run it locally, we can use the LocalWebservice class to create a local deployment configuration.

#+begin_src python
from azureml.core.webservice import LocalWebservice
# ...
deployment_config = LocalWebservice.deploy_configuration(port=6789)
#+end_src

Then, we can create a deployment as the following
#+begin_src python
service = Model.deploy(
    workspace=workspace,
    name="<name-of-service>",
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config,
    overwrite=True,
    deployment_target=None
)
service.wait_for_deployment(show_output=True)
print(service.scoring_uri)
#+end_src

Create a new script called ~deploy_service.py~ in the same folder as the package script. Copy most of the contents from the package script and change it such that we deploy the model locally.

* Deploying to Azure ML Compute
However, if we want to make the model available to everyone, we need to deploy it to the cloud. Defining a small kubernetes cluster is fairly easy in Azure ML. If you have a free subscriptions, your vCPU quota is usually quite small, so we should limit ourselves to using a single node. Standard production clusters require a minimum of three nodes, so you should set the cluster purpose to DEV_TEST.

Something like this should do the trick.
#+begin_src python
from azureml.core.compute import ComputeTarget, AksCompute
from azureml.core.compute_target import ComputeTargetException
# ...

inference_cluster_name = "my-aks"
try:
    aks_target = AksCompute(workspace, name=inference_cluster_name)
except ComputeTargetException:
    provisioning_config = AksCompute.provisioning_configuration(
        vm_size='Standard_D2as_v4', # The smallest size
        agent_count = 1,
        cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST # Needed for having less than three nodes
    )
    aks_target = ComputeTarget.create(
        workspace = workspace,
        name = inference_cluster_name,
        provisioning_configuration = provisioning_config
    )
    aks_target.wait_for_completion(show_output = True)
#+end_src
A tip is to extract this as a function in the ~utils.py~ and use the EnvironmentVariable to configure the parameters.

We also need to create a new deployment configuration for our AKS cluster. We should update our ~deploy_service.py~ to take an new deployment config and the new compute target
#+begin_src python
from azureml.core.webservice import AksWebservice
# ...
model = Model(workspace, name=env_vars.model_name)
deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)

service = Model.deploy(
    workspace=workspace,
    name=env_vars.service_name,
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config,
    overwrite=True,
    deployment_target=aks_target
)
service.wait_for_deployment(show_output=True)
print('uri', service.scoring_uri)
print('key', service.get_keys()[0]) # Your service is per default protected by key authentication
#+end_src

Running this script can take up to 10 minutes, with most of the time being spent setting up the compute cluster. However, since we didn't change the scoring or environment, that build should be almost instant.

Use the uri and key to send some test request to your new service. If you lost them you can always see them in the studio. Test that your service works using our neat testing script

* Final Thougths
So far, we have created a very basic POC and we have already deployed it as a webservice running in a mock production environment. Your little project can now be considered as being MLOps Level 0. The next step is now to move towards MLOps Level 1, by making our training automated. In the step after that, we will move towards running training in GitHub.
